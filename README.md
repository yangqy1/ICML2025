# ICML2025

Table 1: Compare with more baselines on LongBench. 'Previous layer' uses the attention of the previous layer as prediction and selects top-k positions as critical tokens. Similarly, 'Previous token' uses the attention of the previous token in each layer as attention prediction. For baseline [1], [2], [4], we report the results reported in their papers. For baseline [3] and [5], we use their official codes.

![image](https://github.com/user-attachments/assets/8e018416-2b34-47de-b551-242500836dd6)						
![image](https://github.com/user-attachments/assets/242b5a01-c2c0-45d0-8ec5-331cbdb82ff4)


Table 2: Evaluation on InfiniBench with up to 200K context length.

<img src="https://github.com/user-attachments/assets/da878b0f-c9a2-4407-9ac6-d5f87b65eac0" alt="描述" width="500" height="100" />

Table 3: Evaluation on long-CoT task with average decode length 1716.
![image](https://github.com/user-attachments/assets/f9ddfa83-4375-438a-83e8-e6b9f303e7ae)

Figure 1: Revision of Algorithm 1 with clarification of the behavior of the $A_H$ Update and position Expand functions.
![image](https://github.com/user-attachments/assets/38d9ab3f-cf81-48b2-bb64-c998f6d454b7)

Table 4: 
![image](https://github.com/user-attachments/assets/95930757-c134-4cc5-92d1-47e02229e168)

Figure 2: Accuracy of the predictor with different ratios of training data.
![image](https://github.com/user-attachments/assets/02c80745-4eb3-449b-ba37-a67a678b48c2)
